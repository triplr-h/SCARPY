Tue, 10 Mar 2020 22:25:48 log.py[line:146] INFO Scrapy 1.6.0 started (bot: Search_WebHub)
Tue, 10 Mar 2020 22:25:48 log.py[line:149] INFO Versions: lxml 4.5.0.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
Tue, 10 Mar 2020 22:25:48 crawler.py[line:38] INFO Overridden settings: {'BOT_NAME': 'Search_WebHub', 'CLOSESPIDER_ITEMCOUNT': 5, 'CONCURRENT_REQUESTS': 20, 'DEPTH_PRIORITY': 1, 'DOWNLOAD_DELAY': 1, 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'Search_WebHub.spiders', 'REDIRECT_ENABLED': False, 'ROBOTSTXT_OBEY': True, 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue', 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue', 'SPIDER_MODULES': ['Search_WebHub.spiders']}
Tue, 10 Mar 2020 22:25:48 telnet.py[line:60] INFO Telnet Password: 87f53def964e5c93
Tue, 10 Mar 2020 22:25:49 middleware.py[line:48] INFO Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.closespider.CloseSpider',
 'scrapy.extensions.logstats.LogStats']
Tue, 10 Mar 2020 22:25:51 middleware.py[line:48] INFO Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Search_WebHub.middlewares.UserAgentMiddleware',
 'Search_WebHub.middlewares.CookiesMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
Tue, 10 Mar 2020 22:25:51 middleware.py[line:48] INFO Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
Tue, 10 Mar 2020 22:26:22 _legacy.py[line:154] CRITICAL Unhandled error in Deferred:
Tue, 10 Mar 2020 22:26:22 _legacy.py[line:154] CRITICAL 
Traceback (most recent call last):
  File "G:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "G:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "G:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "G:\Anaconda3\lib\site-packages\scrapy\core\engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "G:\Anaconda3\lib\site-packages\scrapy\core\scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "G:\Anaconda3\lib\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "G:\Anaconda3\lib\site-packages\scrapy\middleware.py", line 35, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "G:\Anaconda3\lib\site-packages\scrapy\utils\misc.py", line 144, in create_instance
    return objcls(*args, **kwargs)
  File "G:\Git_code\SCRAPY\Search_WebHub\Search_WebHub\pipelines.py", line 24, in __init__
    self.PhRes.create_indexes([idx])
  File "G:\Anaconda3\lib\site-packages\pymongo\collection.py", line 1841, in create_indexes
    with self._socket_for_writes(session) as sock_info:
  File "G:\Anaconda3\lib\site-packages\pymongo\collection.py", line 195, in _socket_for_writes
    return self.__database.client._socket_for_writes(session)
  File "G:\Anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1266, in _socket_for_writes
    server = self._select_server(writable_server_selector, session)
  File "G:\Anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1253, in _select_server
    server = topology.select_server(server_selector)
  File "G:\Anaconda3\lib\site-packages\pymongo\topology.py", line 235, in select_server
    address))
  File "G:\Anaconda3\lib\site-packages\pymongo\topology.py", line 193, in select_servers
    selector, server_timeout, address)
  File "G:\Anaconda3\lib\site-packages\pymongo\topology.py", line 209, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
